# CSDNWRITERS
* [paper summary](https://www.jianshu.com/p/e73851f32c9f)  
* [csdn writer](https://blog.csdn.net/yingpeng_zhong?t=1)  
* [缁艰堪浣17](https://blog.csdn.net/wspba)  
* [缂╃缁缃缁瀹楠绠寸ョ](https://blog.csdn.net/jason19966)  
* [several paper](https://blog.csdn.net/cookie_234?t=1)  
* [板ぇ澶](https://blog.csdn.net/liujianlin01)  
# ARTICLES
* [](https://www.jiqizhixin.com/articles/2018-06-01-11)  
* [妯″缂╄烘剁稿](https://www.jishux.com/p/4007c8b22f2c7083)  
* [绁缁缃缁妯″缂╀甯歌规](https://blog.csdn.net/LiJiancheng0614/article/details/79478792?utm_source=blogxgwz1)  
* [prune缃缁](https://blog.csdn.net/SIGAI_CSDN/article/details/80803956?utm_source=blogxgwz8)  
* [github summary](https://github.com/Ewenwan/MVision/tree/master/CNN/Deep_Compression/quantization)
# PRUNING
* [Soft Weight-Sharing for Neural Network Compression ](http://cn.arxiv.org/abs/1702.04008)
* [Channel Pruning for Accelerating Very Deep Neural Networks](https://arxiv.org/abs/1707.06168)
* [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)
* [MODEL COMPRESSION WITH GENERATIVE ADVERSARIAL NETWORKS](https://openreview.net/forum?id=Byxz4n09tQ)
* [Dynamic Network Surgery for Efficient DNNs](https://arxiv.org/abs/1608.04493)
  * 绔涓昏哄浠ユ㈠17
    * 浼缁浼ら瑕杩ヨ涓ㄦ17
  * maskヤ唬琛ㄧ舵锛姣杞╃mask拌＄虹loss存板
  * 17杩存板17兼存mask(a<sub>k</sub>b<sub></sub>)
  * 涓轰17璁缁浣?灏よョ姒垮炬舵?风Н灞ㄨュ寮17
* [To prune, or not to prune: exploring the efficacy of pruning for model compression](http://cn.arxiv.org/abs/1710.01878)
* [Learning both Weights and Connections for Efficient Neural Networks](http://cn.arxiv.org/abs/1506.02626)
* [Sensitivity Based Network Pruning: A Modern Perspective](http://users.cecs.anu.edu.au/~Tom.Gedeon/conf/ABCs2018/paper/ABCs2018_paper_135.pdf)
  * 瀹涔浜涓涓ㄥ浼惰＄绁缁杩ユ搴寮
  * 杩绱璁″ㄥ跺瀵煎规璇宸
* [A pruning based method to learn both weights and connections for LSTM](https://nlp.stanford.edu/courses/cs224n/2015/reports/2.pdf)
  * 瀹璁缁
  * ф17肩瀵瑰17煎ぇ灏17
* [Second order derivatives for network pruning: Optimal Brain Surgeon](https://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf)
  * 寰锛姹浜跺瀵间涓洪瑕17
# QUANTIZATION
  ## binarization
  * [binary neural network](https://arxiv.org/abs/1602.02830)
  * [XNOR-net](https://arxiv.org/abs/1603.05279)
  * [HORQ-net](https://arxiv.org/abs/1708.08687)
  * [ternary neural network](https://arxiv.org/abs/1705.01462)
  * [DeRefa-net](https://arxiv.org/abs/1606.06160)
  * [YodaNN](https://arxiv.org/abs/1606.05487)
  * [BinaryRelax- A Relaxation Approach For Training Deep Neural Networks With Quantized Weights](http://cn.arxiv.org/abs/1801.06313)
  ## fixed-point quantization
  * [fixed-point factorized network](https://arxiv.org/abs/1611.01972)
  ## quantization method
  * [incremental network quantization](https://arxiv.org/abs/1702.03044)
  * [Hashed-Net](https://arxiv.org/abs/1702.00758)
  * [Quantized Convolutional Neural Networks for Mobile Devices](https://arxiv.org/abs/1512.06473)
  ## quantization article
  * [Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM](https://arxiv.org/abs/1707.09870v1)
  *  [summary](https://blog.csdn.net/u012101561/article/details/80868352?utm_source=blogxgwz34)
  * [Scalable Methods for 8-bit Training of Neural Networks](https://arxiv.org/abs/1707.09870v1)
    * 待复现
  * [Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM](https://arxiv.org/abs/1805.11046v1)
    * 待复现
# NET
* [GoogLeNet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)
* [SqueezeNet](https://arxiv.org/abs/1602.07360)
* [MobileNets](https://arxiv.org/abs/1704.04861)
* [ShuffleNet](https://arxiv.org/abs/1707.01083)
